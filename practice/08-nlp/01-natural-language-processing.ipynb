{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "964b1f96-5d14-4366-875c-cfc2f545ef49",
   "metadata": {},
   "source": [
    "# Task to the topic Natural Language Processing (NLP)\n",
    "\n",
    "You task is to implement all the steps of NLP algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dd07401-2f1c-4be6-9db5-519688fd061a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/mehalyna/cooltest.git\n",
      "  Cloning https://github.com/mehalyna/cooltest.git to /tmp/pip-req-build-o8kvts1b\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/mehalyna/cooltest.git /tmp/pip-req-build-o8kvts1b\n",
      "  Resolved https://github.com/mehalyna/cooltest.git to commit 630c96f2d3300782279879d5d13e6c1aaabf3c75\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /home/m/Projects/python/ai-course/practice/venv/lib/python3.11/site-packages (from cooltest==26.22) (2.2.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/mehalyna/cooltest.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc33134-66f8-45a7-be2b-26f5c56ad2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass\n"
     ]
    }
   ],
   "source": [
    "from cooltest.test_cool_1 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432bef79-705d-49d1-81c8-0573575c3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install all the needed modules\n",
    "#import spacy library\n",
    "\n",
    "#load core english library\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997f6e97-1b57-4ea3-a556-a774b7d3c3b5",
   "metadata": {},
   "source": [
    "1. Segmentation.\n",
    "\n",
    "Break the entire document down into its constituent sentences. You can do this by segmenting the article along with its punctuations like full stops and commas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3927b5ab-8733-46f8-a635-f579d6bfb4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Natural Language Processing (NLP) is a part of AI (artificial intelligence) that deals with understanding and processing of human language. \n",
    "In real time, majority of data exists in the unstructured form such us text, videos, images. \n",
    "Mass of data in unstructured category, will be in textual form. \n",
    "To process this textual data's with machine learning algorithms, NLP comes in to play.\n",
    "NLP use cases are Language translation, Speech recognition, Hiring and Recruitment, Chat Bot, Sentimental analysis and so on.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343caa2c-5746-4d53-9ad7-edd59a1bc473",
   "metadata": {},
   "source": [
    "Write the body of function ```get_segment``` which would be used for the segmentation of the given ```text```. Function will return the sequence of the segments as the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2522559-7fdf-454b-8d49-f7b4a81a2d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "@test_get_segment\n",
    "def get_segment(some_text):\n",
    "  #take string\n",
    "  \n",
    "  #to print sentences\n",
    "  \n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e6578b-be28-4651-8ab1-db12daef1c6c",
   "metadata": {},
   "source": [
    "2. Tokenizing\n",
    "\n",
    "Define the body of the function ```get_tokens()``` that would be used for breaking down given sentence into its constituent words (tokens). The function will return the sequence of the tokens as the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dec633-3ff0-4efc-814a-d194cfa5b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Tokenizer\n",
    "\n",
    "text = \"Natural Language Processing (NLP) is a part of AI (artificial intelligence) that deals with understanding and processing of human language.\"\n",
    "\n",
    "@test_get_tokens\n",
    "def get_tokens(some_text):\n",
    "  \n",
    "  return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52750a09-2f1b-4d55-a85a-6e6ba7344706",
   "metadata": {},
   "source": [
    "3. Removing Stop Words\n",
    "\n",
    "Your next task is getting rid of non-essential words, which add little meaning to the statement ```new_tokens``` and are just there to make your statement sound more cohesive. Words such as *was, in, is, and, the*, are called stop words and can be removed.\n",
    "Write the body of function ```remove_stop_words()``` which will take the parameter sequence with tokens (```some_tokens```) and return as the result the sequence of tokens without non-essential words (*stop words*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3050c286-58d4-419f-a24b-4ece09201526",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import module for stop words removing\n",
    "\n",
    "\n",
    "@test_remove_stop_words\n",
    "def remove_stop_words(some_tokens):\n",
    "  \n",
    "  return filtered_sentence \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6837580-48ec-4824-b130-26108a77133e",
   "metadata": {},
   "source": [
    "4. Stemming and Lemmatization\n",
    "\n",
    "Now you have to obtain the Root Stem of a word. Root Stem gives the new base form of a word that is present in the dictionary and from which the word is derived. You can also identify the base word for different words, - lemma.\n",
    "\n",
    "Write the body ```stemm_lemmatization()``` of function which should return the list of lemmas of given parameter text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f630062-bb45-4fe7-8b31-4085380a5d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "@test_stemm_lemmatization\n",
    "def stemm_lemmatization(doc):\n",
    "\n",
    "  return list_lemma\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d431ce-5e17-47ae-b31d-0119d4ab2ade",
   "metadata": {},
   "source": [
    "5. Part of Speech Tagging:\n",
    "\n",
    "Now, you have to define the body of function ```part_of()``` that will take the sentence of text as the parameter. The function shoul return the dictionary with words as keys and appropriate tags (parts of speech) as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d814be-dab9-432f-bf87-e1aa3535ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@test_part_of\n",
    "def part_of(doc):\n",
    "  \n",
    "  return word_tags\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
