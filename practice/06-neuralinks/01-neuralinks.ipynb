{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Implement a Perceptron from Scratch\n",
    "\n",
    "**Problem:** Creating a Perceptron Model for XOR Classification\n",
    "\n",
    "**Task:** Develop a simple perceptron model using Python and NumPy to perform binary classification on the XOR problem.\n",
    "\n",
    "**Description:** The XOR problem involves classifying a set of input pairs `(x1, x2)` into two classes: 1 if the inputs are different (`0, 1` or `1, 0`), and 0 if the inputs are the same (`0, 0` or `1, 1`). A perceptron is a simple neural network model that can learn linear decision boundaries. In this task, you will create a perceptron, train it on the XOR dataset, and observe its classification performance.\n",
    "\n",
    "**Steps:**\n",
    "1. Implement a Perceptron class with methods for initialization, activation, prediction, and training.\n",
    "2. Initialize the perceptron's weights and bias with random values.\n",
    "3. Define the XOR dataset: input features and corresponding labels.\n",
    "4. Use the perceptron's `train` method to update weights and bias using the perceptron learning rule.\n",
    "5. Test the trained perceptron on the XOR dataset and print its predictions.\n",
    "\n",
    "**Expected Output:**\n",
    "For each input pair `(x1, x2)` in the XOR dataset, the perceptron should predict the corresponding class label (`0` or `1`). The final output should demonstrate the perceptron's ability to learn and classify the XOR problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0], Prediction: 1\n",
      "Input: [0 1], Prediction: 1\n",
      "Input: [1 0], Prediction: 0\n",
      "Input: [1 1], Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size):\n",
    "        \"\"\"\n",
    "        Initializes a simple perceptron model.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Number of input features.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # where w_[0] is bias and w_[1:] are weights\n",
    "        rgen = np.random.RandomState(1)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + self.input_size)\n",
    "\n",
    "\n",
    "    def activation(self, x):\n",
    "        \"\"\"\n",
    "        Activation function (Step function).\n",
    "\n",
    "        Args:\n",
    "            x (float): Input value.\n",
    "\n",
    "        Returns:\n",
    "            int: 1 if x >= 0, else 0.\n",
    "        \"\"\"\n",
    "        return int(1) if x >= 0 else int(0)\n",
    "\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts the output label using the perceptron model.\n",
    "\n",
    "        Args:\n",
    "            x (ndarray): Input features.\n",
    "\n",
    "        Returns:\n",
    "            int: Predicted label (1 or 0).\n",
    "        \"\"\"\n",
    "\n",
    "        return self.activation(np.dot(x, self.w_[1:]) + self.w_[0])\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, X, y, num_epochs, learning_rate):\n",
    "        \"\"\"\n",
    "        Trains the perceptron model on the given dataset using the perceptron learning rule.\n",
    "\n",
    "        Args:\n",
    "            X (ndarray): Input features of the dataset.\n",
    "            y (ndarray): Ground truth labels of the dataset.\n",
    "            num_epochs (int): Number of training epochs.\n",
    "            learning_rate (float): Learning rate for weight update.\n",
    "        \"\"\"\n",
    "\n",
    "        for _ in range(num_epochs):\n",
    "            for xi, target in zip(X, y):\n",
    "                update = learning_rate * (target - self.predict(xi))\n",
    "                self.w_[1:] += update * xi\n",
    "                self.w_[0] += update\n",
    "        return None\n",
    "\n",
    "# XOR dataset: Input features and corresponding labels\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Create and train the perceptron model\n",
    "perceptron = Perceptron(input_size=2)\n",
    "perceptron.train(X, y, num_epochs=1000, learning_rate=0.1)\n",
    "\n",
    "# Test the trained model\n",
    "test_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "for data in test_data:\n",
    "    prediction = perceptron.predict(data)\n",
    "    print(f\"Input: {data}, Prediction: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Creating a visualization of common activation functions (sigmoid, ReLU, and tanh) using Python and Matplotlib\n",
    "\n",
    "**Problem:** Activation Function Visualization\n",
    "\n",
    "**Task:** Create a Visualization of Common Activation Functions\n",
    "\n",
    "**Description:** Activation functions are crucial components in neural networks that introduce non-linearity and enable the network to learn complex patterns. This task involves visualizing the behavior of common activation functions: sigmoid, ReLU (Rectified Linear Unit), and tanh (hyperbolic tangent). By plotting their graphs, you will gain insights into how they transform input values.\n",
    "\n",
    "**Steps:**\n",
    "1. Implement the activation functions `sigmoid`, `relu`, and `tanh`.\n",
    "2. Generate a range of input values for plotting.\n",
    "3. Calculate the output values for each activation function using the provided input range.\n",
    "4. Use Matplotlib to create a plot that shows the graphs of all three activation functions on the same plot.\n",
    "5. Add labels, title, and legend to the plot to make it informative.\n",
    "\n",
    "**Expected Output:**\n",
    "A single plot should display the graphs of the sigmoid, ReLU, and tanh activation functions. The x-axis should represent the input values, and the y-axis should represent the output values. Each activation function's graph should have a distinctive shape that showcases its unique characteristics.\n",
    "\n",
    "**Tips:**\n",
    "- Sigmoid: S-shaped curve that maps input values to a range between 0 and 1.\n",
    "- ReLU: Linear for positive values and zero for negative values, creating a hinge-like shape.\n",
    "- tanh: S-shaped curve that maps input values to a range between -1 and 1.\n",
    "\n",
    "**Learning Objectives:**\n",
    "This task will help you understand the effects of different activation functions on input data transformation. You'll observe how these functions introduce non-linearity, saturate, or allow only positive outputs, which are essential aspects of neural network behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "\n",
    "    Args:\n",
    "        x (float): Input value.\n",
    "\n",
    "    Returns:\n",
    "        float: Output of the sigmoid function.\n",
    "    \"\"\"\n",
    "    return result\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit (ReLU) activation function.\n",
    "\n",
    "    Args:\n",
    "        x (float): Input value.\n",
    "\n",
    "    Returns:\n",
    "        float: Output of the ReLU function.\n",
    "    \"\"\"\n",
    "    return result\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    Hyperbolic tangent (tanh) activation function.\n",
    "\n",
    "    Args:\n",
    "        x (float): Input value.\n",
    "\n",
    "    Returns:\n",
    "        float: Output of the tanh function.\n",
    "    \"\"\"\n",
    "    return result\n",
    "\n",
    "# Create x values for plotting\n",
    "\n",
    "\n",
    "# Calculate y values for each activation function\n",
    "\n",
    "\n",
    "# Plot the activation functions\n",
    "\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Hyperparameter Tuning\n",
    "\n",
    "**Problem:** Hyperparameter Tuning for Neural Network\n",
    "\n",
    "**Task:** Experiment with Different Hyperparameters to Optimize Neural Network Performance\n",
    "\n",
    "**Description:** Hyperparameters are crucial settings that impact the performance of neural network models. This task involves exploring the effects of different hyperparameters, such as learning rate and number of hidden units, on the performance of a neural network model. By conducting experiments with various hyperparameter values, you will gain insights into how these settings influence the training process and final results.\n",
    "\n",
    "**Steps:**\n",
    "1. Implement a function to create a neural network model with a specified number of hidden units.\n",
    "2. Implement a function to train the neural network model on the provided training data.\n",
    "3. Define the hyperparameters to experiment with: learning rates and hidden units.\n",
    "4. Loop through different combinations of hyperparameters and perform the following:\n",
    "   - Create a new neural network model with the specified hyperparameters.\n",
    "   - Train the model using the training data and provided hyperparameters.\n",
    "   - Print the results of each experiment, including learning rate, hidden units, and training performance.\n",
    "\n",
    "**Expected Output:**\n",
    "For each combination of hyperparameters, you will observe the learning process of the neural network as it trains on the provided data. The output will include training progress updates and performance metrics such as loss and accuracy.\n",
    "\n",
    "**Tips:**\n",
    "- Experiment with a range of learning rates (e.g., 0.001, 0.01, 0.1) and hidden units (e.g., 4, 8, 16).\n",
    "- Observe how different learning rates affect convergence speed and final performance.\n",
    "- Notice the impact of changing the number of hidden units on model complexity and overfitting.\n",
    "\n",
    "**Learning Objectives:**\n",
    "This task will help you understand the significance of hyperparameters in neural networks and their effects on model training and performance. You will gain hands-on experience in tuning hyperparameters and making informed decisions to optimize your neural network models.\n",
    "\n",
    "_Note._ In code, we define two functions:\n",
    "\n",
    "- `create_neural_network`: Creates a neural network model with a specified number of hidden units and compiles it using the Adam optimizer.\n",
    "- `train_neural_network`: Trains the given neural network model on the provided training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SGD\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def create_neural_network(input_dim, hidden_units):\n",
    "    \"\"\"\n",
    "    Create a neural network model with a specified number of hidden units.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Number of input features.\n",
    "        hidden_units (int): Number of units in the hidden layer.\n",
    "\n",
    "    Returns:\n",
    "        model (Sequential): Compiled neural network model.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_neural_network(model, X_train, y_train, num_epochs, batch_size):\n",
    "    \"\"\"\n",
    "    Train the neural network model on the training data.\n",
    "\n",
    "    Args:\n",
    "        model (Sequential): Compiled neural network model.\n",
    "        X_train (ndarray): Training input features.\n",
    "        y_train (ndarray): Training labels.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        batch_size (int): Batch size for training.\n",
    "    \"\"\"\n",
    "    return None\n",
    "\n",
    "# Example data and labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters to experiment with\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Text Classification with Word Embeddings\n",
    "\n",
    "**Problem:** Text Classification with Word Embeddings\n",
    "\n",
    "**Task:** Train a Neural Network for Sentiment Analysis Using Word Embeddings\n",
    "\n",
    "**Description:** Sentiment analysis is the process of determining the sentiment or emotional tone expressed in a piece of text. This task involves training a simple neural network to perform sentiment analysis on a dataset of text reviews. Word embeddings will be utilized to represent words in a dense vector space, capturing semantic relationships between words. The trained model will then predict whether a given text review has a positive or negative sentiment.\n",
    "\n",
    "**Steps:**\n",
    "1. Load and preprocess the dataset of text reviews and their corresponding sentiment labels (positive or negative).\n",
    "2. Tokenize the text data to convert it into sequences of numerical values (word indices).\n",
    "3. Pad the sequences to ensure uniform length for neural network input.\n",
    "4. Build a neural network model with an embedding layer to learn word representations and a dense output layer for binary sentiment classification.\n",
    "5. Train the model using the tokenized and padded text data, aiming to minimize the binary cross-entropy loss.\n",
    "6. Evaluate the model's performance using evaluation metrics such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "**Expected Output:**\n",
    "A trained neural network model capable of predicting the sentiment (positive or negative) of text reviews. This model should be able to process new text reviews and classify their sentiments accurately.\n",
    "\n",
    "**Tips:**\n",
    "- Use a well-known dataset for sentiment analysis, such as IMDb movie reviews.\n",
    "- Experiment with different hyperparameters like embedding dimension, number of hidden layers, and batch size.\n",
    "- Visualize the training progress by plotting the loss and accuracy curves over epochs.\n",
    "\n",
    "**Learning Objectives:**\n",
    "By completing this task, you'll gain practical experience in natural language processing and sentiment analysis. You'll understand the importance of word embeddings in capturing semantic information from text and how neural networks can be effectively used for text classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import load_model\n",
    "\n",
    "def train_sentiment_analysis_model(texts, labels, max_words, embedding_dim, num_epochs, batch_size):\n",
    "    \"\"\"\n",
    "    Train a neural network model for sentiment analysis using word embeddings.\n",
    "\n",
    "    Args:\n",
    "        texts (list): List of text reviews.\n",
    "        labels (list): List of corresponding sentiment labels (0 or 1).\n",
    "        max_words (int): Maximum number of words to tokenize.\n",
    "        embedding_dim (int): Dimension of word embeddings.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        batch_size (int): Batch size for training.\n",
    "    \"\"\"\n",
    "\n",
    "    return None\n",
    "\n",
    "# Example data: text reviews and sentiment labels\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "\n",
    "\n",
    "# Train the sentiment analysis model\n",
    "\n",
    "\n",
    "\n",
    "# Test the model\n",
    "\n",
    "# Load the trained model\n",
    "# Load your model file\n",
    "\n",
    "\n",
    "# Example new text reviews\n",
    "\n",
    "\n",
    "# Tokenize and pad the new text reviews\n",
    "\n",
    "\n",
    "\n",
    "# Use the trained model to predict sentiments\n",
    "\n",
    "\n",
    "# Convert the predictions to binary labels (0 or 1)\n",
    "\n",
    "\n",
    "# Print the predicted sentiments for new text reviews\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
